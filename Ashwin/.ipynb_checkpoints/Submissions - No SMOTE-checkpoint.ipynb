{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#visualization\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CRASH DATE', 'CRASH TIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE',\n",
      "       'LONGITUDE', 'LOCATION', 'ON STREET NAME', 'CROSS STREET NAME',\n",
      "       'OFF STREET NAME', 'NUMBER OF PERSONS INJURED',\n",
      "       'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED',\n",
      "       'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED',\n",
      "       'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED',\n",
      "       'NUMBER OF MOTORIST KILLED', 'CONTRIBUTING FACTOR VEHICLE 1',\n",
      "       'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3',\n",
      "       'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5',\n",
      "       'COLLISION_ID', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2',\n",
      "       'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5',\n",
      "       'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'NUMTIME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"MVC.csv\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['BOROUGH','NUMTIME','LONGITUDE','LATITUDE','CONTRIBUTING FACTOR VEHICLE 1','NUMBER OF PERSONS KILLED','NUMBER OF PERSONS INJURED','NUMBER OF PEDESTRIANS KILLED','NUMBER OF PEDESTRIANS INJURED']]\n",
    "data['PERSONS_AFFECTED'] = data['NUMBER OF PERSONS KILLED'] + data['NUMBER OF PERSONS INJURED']\n",
    "data['PEDESTRIANS_AFFECTED'] = data['NUMBER OF PEDESTRIANS KILLED'] + data['NUMBER OF PEDESTRIANS INJURED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_dummy = data.BOROUGH.str.get_dummies()\n",
    "data = pd.concat([data,borough_dummy],axis=1)\n",
    "\n",
    "data = data.dropna()\n",
    "data = data[data.LONGITUDE !=0]\n",
    "\n",
    "data['PERSONS_TF']=0\n",
    "data.loc[data['PERSONS_AFFECTED'] > 0, 'PERSONS_TF']= 1\n",
    "data.loc[data['PERSONS_AFFECTED'] == 0, 'PERSONS_TF']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = data[\"CONTRIBUTING FACTOR VEHICLE 1\"].value_counts(ascending=True)\n",
    "remove = value_counts[value_counts <= 20].index\n",
    "data = data[~data['CONTRIBUTING FACTOR VEHICLE 1'].isin(remove)]\n",
    "\n",
    "data['CONTRIBUTING FACTOR VEHICLE 1'] = data['CONTRIBUTING FACTOR VEHICLE 1'].replace(['Driver Inattention/Distraction','Following Too Closely','Failure to Yield Right-of-Way','Backing Unsafely','Passing Too Closely','Unsafe Lane Changing','Passing or Lane Usage Improper','Turning Improperly','Traffic Control Disregarded','Driver Inexperience','Reaction to Uninvolved Vehicle','Unsafe Speed','Fatigued/Drowsy','Alcohol Involvement','Lost Consciousness','Aggressive Driving/Road Rage','Fell Asleep','Illnes','Drugs (illegal)','Drugs (Illegal)','Texting','Outside Car Distraction','Passenger Distraction','Prescription Medication','Failure to Keep Right','Illness','Other Electronic Device','Cell Phone (hand-Held)','Cell Phone (hands-free)','Eating or Drinking','Pedestrian/Bicyclist/Other Pedestrian Error/Confusion','Physical Disability','Reaction to Other Uninvolved Vehicle','Using On Board Navigation Device','Tinted Windows','Vehicle Vandalism','Cell Phone (hand-held)'],'Human')\n",
    "data['CONTRIBUTING FACTOR VEHICLE 1'] = data['CONTRIBUTING FACTOR VEHICLE 1'].replace(['Other Vehicular','Oversized Vehicle','Brakes Defective','Steering Failure','Tire Failure/Inadequate','Driverless/Runaway Vehicle','Accelerator Defective','Other Lighting Defects','Tow Hitch Defective','Headlights Defective','Windshield Inadequate'],'Vehicular')\n",
    "data['CONTRIBUTING FACTOR VEHICLE 1'] = data['CONTRIBUTING FACTOR VEHICLE 1'].replace(['Pavement Slippery','View Obstructed/Limited','Glare','Obstruction/Debris','Pavement Defective','Animals Action','Lane Marking Improper/Inadequate','Traffic Control Device Improper/Non-Working','Shoulders Defective/Improper'],'Environmental')\n",
    "data['CONTRIBUTING FACTOR VEHICLE 1'] = data['CONTRIBUTING FACTOR VEHICLE 1'].replace(['80'],'Unspecified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_dummy = data['CONTRIBUTING FACTOR VEHICLE 1'].str.get_dummies()\n",
    "data = pd.concat([data,factor_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 529959, 1: 529959})\n"
     ]
    }
   ],
   "source": [
    "pred = list(zip(data['NUMTIME'],data['MANHATTAN'],data['QUEENS'],data['BRONX'],data['STATEN ISLAND'],data['BROOKLYN'],data['Human'],data['Environmental'],data['Vehicular'],data['Unspecified']))\n",
    "resp = data['PERSONS_TF']\n",
    "sm = SMOTE(random_state=42)\n",
    "x_res,y_res = sm.fit_resample(pred,resp)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  5.5min\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 50.3min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.809755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_criterion param_max_depth param_min_samples_leaf  mean_test_score\n",
       "0            gini               1                     10         0.809755\n",
       "1         entropy               4                     20         0.809755\n",
       "2         entropy               3                     60         0.809755\n",
       "3         entropy               3                     50         0.809755\n",
       "4         entropy               3                     30         0.809755\n",
       "5         entropy               3                     20         0.809755\n",
       "6         entropy               3                     10         0.809755\n",
       "7         entropy               2                     60         0.809755\n",
       "8         entropy               2                     50         0.809755\n",
       "9         entropy               2                     30         0.809755"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feat = pred\n",
    "y = resp\n",
    "X_feat_train,X_feat_test, y_feat_train,y_feat_test = train_test_split(X_feat,y,test_size=0.2)\n",
    "\n",
    "dtc = RandomForestClassifier()\n",
    "grid={'max_depth':[1,2,3,4,5,10,20,30],\n",
    "        'criterion':['gini','entropy'],\n",
    "        'min_samples_leaf':[10,20,30,50,60]}\n",
    "\n",
    "gs = GridSearchCV(dtc,grid,cv=3,verbose=True,return_train_score=False,n_jobs=-1)\n",
    "gs.fit(X_feat,y)\n",
    "\n",
    "scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
    "                                            ).sort_values('mean_test_score',\n",
    "                                                          ascending=False).reset_index().drop(['index'],axis=1)\n",
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89    105961\n",
      "           1       0.00      0.00      0.00     24933\n",
      "\n",
      "    accuracy                           0.81    130894\n",
      "   macro avg       0.40      0.50      0.45    130894\n",
      "weighted avg       0.66      0.81      0.72    130894\n",
      "\n",
      "Null accuracy on the test set:  0.19048237505156845\n",
      "Sensitivity/Recall (TPR):  0.8095176249484316\n",
      "Precision (PPV):  0.8095176249484316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[105961,      0],\n",
       "       [ 24933,      0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_feat, y, test_size=0.2)\n",
    "\n",
    "\n",
    "t = RandomForestClassifier(criterion='entropy',max_depth=4,min_samples_leaf=20)\n",
    "\n",
    "t.fit(X_train,y_train)\n",
    "y_test_pred = t.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\\n\", metrics.classification_report(y_test,y_test_pred))\n",
    "print(\"Null accuracy on the test set: \",y_test.mean())\n",
    "print(\"Sensitivity/Recall (TPR): \",metrics.recall_score(y_test,y_test_pred,average='micro'))\n",
    "print(\"Precision (PPV): \", metrics.precision_score(y_test,y_test_pred,average='micro'))\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54444197\n",
      "Iteration 2, loss = 0.48603236\n",
      "Iteration 3, loss = 0.48467893\n",
      "Iteration 4, loss = 0.48409762\n",
      "Iteration 5, loss = 0.48380687\n",
      "Iteration 6, loss = 0.48366462\n",
      "Iteration 7, loss = 0.48350209\n",
      "Iteration 8, loss = 0.48340219\n",
      "Iteration 9, loss = 0.48352559\n",
      "Iteration 10, loss = 0.48342275\n",
      "Iteration 11, loss = 0.48349731\n",
      "Iteration 12, loss = 0.48339430\n",
      "Iteration 13, loss = 0.48334321\n",
      "Iteration 14, loss = 0.48348175\n",
      "Iteration 15, loss = 0.48340809\n",
      "Iteration 16, loss = 0.48350228\n",
      "Iteration 17, loss = 0.48332811\n",
      "Iteration 18, loss = 0.48331285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53888530\n",
      "Iteration 2, loss = 0.48566952\n",
      "Iteration 3, loss = 0.48481179\n",
      "Iteration 4, loss = 0.48415829\n",
      "Iteration 5, loss = 0.48389181\n",
      "Iteration 6, loss = 0.48375682\n",
      "Iteration 7, loss = 0.48360556\n",
      "Iteration 8, loss = 0.48358718\n",
      "Iteration 9, loss = 0.48355527\n",
      "Iteration 10, loss = 0.48356809\n",
      "Iteration 11, loss = 0.48347653\n",
      "Iteration 12, loss = 0.48342445\n",
      "Iteration 13, loss = 0.48347095\n",
      "Iteration 14, loss = 0.48344530\n",
      "Iteration 15, loss = 0.48348573\n",
      "Iteration 16, loss = 0.48341510\n",
      "Iteration 17, loss = 0.48343398\n",
      "Iteration 18, loss = 0.48334642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54845204\n",
      "Iteration 2, loss = 0.48627973\n",
      "Iteration 3, loss = 0.48524183\n",
      "Iteration 4, loss = 0.48477656\n",
      "Iteration 5, loss = 0.48455454\n",
      "Iteration 6, loss = 0.48431399\n",
      "Iteration 7, loss = 0.48423456\n",
      "Iteration 8, loss = 0.48419665\n",
      "Iteration 9, loss = 0.48411154\n",
      "Iteration 10, loss = 0.48412558\n",
      "Iteration 11, loss = 0.48406872\n",
      "Iteration 12, loss = 0.48408368\n",
      "Iteration 13, loss = 0.48411092\n",
      "Iteration 14, loss = 0.48402219\n",
      "Iteration 15, loss = 0.48404923\n",
      "Iteration 16, loss = 0.48397714\n",
      "Iteration 17, loss = 0.48404310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49224908\n",
      "Iteration 2, loss = 0.49028370\n",
      "Iteration 3, loss = 0.49125588\n",
      "Iteration 4, loss = 0.49017371\n",
      "Iteration 5, loss = 0.49141263\n",
      "Iteration 6, loss = 0.49047800\n",
      "Iteration 7, loss = 0.49004702\n",
      "Iteration 8, loss = 0.48972558\n",
      "Iteration 9, loss = 0.48902152\n",
      "Iteration 10, loss = 0.48870654\n",
      "Iteration 11, loss = 0.48926695\n",
      "Iteration 12, loss = 0.48753154\n",
      "Iteration 13, loss = 0.48824111\n",
      "Iteration 14, loss = 0.48649679\n",
      "Iteration 15, loss = 0.48681090\n",
      "Iteration 16, loss = 0.48651051\n",
      "Iteration 17, loss = 0.48603345\n",
      "Iteration 18, loss = 0.48564104\n",
      "Iteration 19, loss = 0.48540600\n",
      "Iteration 20, loss = 0.48605983\n",
      "Iteration 21, loss = 0.48498875\n",
      "Iteration 22, loss = 0.48530477\n",
      "Iteration 23, loss = 0.48477912\n",
      "Iteration 24, loss = 0.48456018\n",
      "Iteration 25, loss = 0.48458201\n",
      "Iteration 26, loss = 0.48413259\n",
      "Iteration 27, loss = 0.48414162\n",
      "Iteration 28, loss = 0.48402834\n",
      "Iteration 29, loss = 0.48358722\n",
      "Iteration 30, loss = 0.48348356\n",
      "Iteration 31, loss = 0.48364461\n",
      "Iteration 32, loss = 0.48331579\n",
      "Iteration 33, loss = 0.48329013\n",
      "Iteration 34, loss = 0.48316303\n",
      "Iteration 35, loss = 0.48301286\n",
      "Iteration 36, loss = 0.48304162\n",
      "Iteration 37, loss = 0.48295797\n",
      "Iteration 38, loss = 0.48287169\n",
      "Iteration 39, loss = 0.48291645\n",
      "Iteration 40, loss = 0.48292747\n",
      "Iteration 41, loss = 0.48275258\n",
      "Iteration 42, loss = 0.48275172\n",
      "Iteration 43, loss = 0.48283124\n",
      "Iteration 44, loss = 0.48276733\n",
      "Iteration 45, loss = 0.48272282\n",
      "Iteration 46, loss = 0.48277674\n",
      "Iteration 47, loss = 0.48273804\n",
      "Iteration 48, loss = 0.48274585\n",
      "Iteration 49, loss = 0.48278139\n",
      "Iteration 50, loss = 0.48272479\n",
      "Iteration 51, loss = 0.48268319\n",
      "Iteration 52, loss = 0.48273842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49192291\n",
      "Iteration 2, loss = 0.49088631\n",
      "Iteration 3, loss = 0.49058950\n",
      "Iteration 4, loss = 0.48966366\n",
      "Iteration 5, loss = 0.48849354\n",
      "Iteration 6, loss = 0.48863100\n",
      "Iteration 7, loss = 0.48824211\n",
      "Iteration 8, loss = 0.48693381\n",
      "Iteration 9, loss = 0.48689027\n",
      "Iteration 10, loss = 0.48641174\n",
      "Iteration 11, loss = 0.48652982\n",
      "Iteration 12, loss = 0.48572200\n",
      "Iteration 13, loss = 0.48523821\n",
      "Iteration 14, loss = 0.48526441\n",
      "Iteration 15, loss = 0.48462001\n",
      "Iteration 16, loss = 0.48487849\n",
      "Iteration 17, loss = 0.48437114\n",
      "Iteration 18, loss = 0.48424743\n",
      "Iteration 19, loss = 0.48381226\n",
      "Iteration 20, loss = 0.48366580\n",
      "Iteration 21, loss = 0.48354932\n",
      "Iteration 22, loss = 0.48344731\n",
      "Iteration 23, loss = 0.48323974\n",
      "Iteration 24, loss = 0.48332567\n",
      "Iteration 25, loss = 0.48313032\n",
      "Iteration 26, loss = 0.48311917\n",
      "Iteration 27, loss = 0.48296127\n",
      "Iteration 28, loss = 0.48298418\n",
      "Iteration 29, loss = 0.48300671\n",
      "Iteration 30, loss = 0.48290068\n",
      "Iteration 31, loss = 0.48296635\n",
      "Iteration 32, loss = 0.48283593\n",
      "Iteration 33, loss = 0.48291816\n",
      "Iteration 34, loss = 0.48290021\n",
      "Iteration 35, loss = 0.48284752\n",
      "Iteration 36, loss = 0.48288021\n",
      "Iteration 37, loss = 0.48287965\n",
      "Iteration 38, loss = 0.48286879\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49477990\n",
      "Iteration 2, loss = 0.49175073\n",
      "Iteration 3, loss = 0.49153115\n",
      "Iteration 4, loss = 0.49147306\n",
      "Iteration 5, loss = 0.48992475\n",
      "Iteration 6, loss = 0.49183025\n",
      "Iteration 7, loss = 0.49003870\n",
      "Iteration 8, loss = 0.48960877\n",
      "Iteration 9, loss = 0.48997209\n",
      "Iteration 10, loss = 0.48872509\n",
      "Iteration 11, loss = 0.48920843\n",
      "Iteration 12, loss = 0.48854878\n",
      "Iteration 13, loss = 0.48787610\n",
      "Iteration 14, loss = 0.48815079\n",
      "Iteration 15, loss = 0.48747851\n",
      "Iteration 16, loss = 0.48691538\n",
      "Iteration 17, loss = 0.48681832\n",
      "Iteration 18, loss = 0.48602440\n",
      "Iteration 19, loss = 0.48609066\n",
      "Iteration 20, loss = 0.48622288\n",
      "Iteration 21, loss = 0.48564044\n",
      "Iteration 22, loss = 0.48525899\n",
      "Iteration 23, loss = 0.48516575\n",
      "Iteration 24, loss = 0.48496011\n",
      "Iteration 25, loss = 0.48448711\n",
      "Iteration 26, loss = 0.48426219\n",
      "Iteration 27, loss = 0.48414369\n",
      "Iteration 28, loss = 0.48396610\n",
      "Iteration 29, loss = 0.48383107\n",
      "Iteration 30, loss = 0.48381751\n",
      "Iteration 31, loss = 0.48378312\n",
      "Iteration 32, loss = 0.48371440\n",
      "Iteration 33, loss = 0.48366002\n",
      "Iteration 34, loss = 0.48367416\n",
      "Iteration 35, loss = 0.48365153\n",
      "Iteration 36, loss = 0.48361876\n",
      "Iteration 37, loss = 0.48356511\n",
      "Iteration 38, loss = 0.48351661\n",
      "Iteration 39, loss = 0.48358089\n",
      "Iteration 40, loss = 0.48359324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53697628\n",
      "Iteration 2, loss = 0.48572041\n",
      "Iteration 3, loss = 0.48469536\n",
      "Iteration 4, loss = 0.48426810\n",
      "Iteration 5, loss = 0.48396156\n",
      "Iteration 6, loss = 0.48368866\n",
      "Iteration 7, loss = 0.48366760\n",
      "Iteration 8, loss = 0.48351806\n",
      "Iteration 9, loss = 0.48360018\n",
      "Iteration 10, loss = 0.48349552\n",
      "Iteration 11, loss = 0.48346257\n",
      "Iteration 12, loss = 0.48351957\n",
      "Iteration 13, loss = 0.48341231\n",
      "Iteration 14, loss = 0.48345152\n",
      "Iteration 15, loss = 0.48342805\n",
      "Iteration 16, loss = 0.48337459\n",
      "Iteration 17, loss = 0.48337552\n",
      "Iteration 18, loss = 0.48339366\n",
      "Iteration 19, loss = 0.48343762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55728546\n",
      "Iteration 2, loss = 0.48538985\n",
      "Iteration 3, loss = 0.48455427\n",
      "Iteration 4, loss = 0.48409288\n",
      "Iteration 5, loss = 0.48381119\n",
      "Iteration 6, loss = 0.48376056\n",
      "Iteration 7, loss = 0.48357775\n",
      "Iteration 8, loss = 0.48353409\n",
      "Iteration 9, loss = 0.48349387\n",
      "Iteration 10, loss = 0.48354271\n",
      "Iteration 11, loss = 0.48347162\n",
      "Iteration 12, loss = 0.48350225\n",
      "Iteration 13, loss = 0.48346599\n",
      "Iteration 14, loss = 0.48351269\n",
      "Iteration 15, loss = 0.48343400\n",
      "Iteration 16, loss = 0.48347351\n",
      "Iteration 17, loss = 0.48338703\n",
      "Iteration 18, loss = 0.48344602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55233065\n",
      "Iteration 2, loss = 0.48624817\n",
      "Iteration 3, loss = 0.48516491\n",
      "Iteration 4, loss = 0.48477811\n",
      "Iteration 5, loss = 0.48437243\n",
      "Iteration 6, loss = 0.48431781\n",
      "Iteration 7, loss = 0.48411118\n",
      "Iteration 8, loss = 0.48412504\n",
      "Iteration 9, loss = 0.48405754\n",
      "Iteration 10, loss = 0.48412314\n",
      "Iteration 11, loss = 0.48407923\n",
      "Iteration 12, loss = 0.48403141\n",
      "Iteration 13, loss = 0.48413235\n",
      "Iteration 14, loss = 0.48401373\n",
      "Iteration 15, loss = 0.48397292\n",
      "Iteration 16, loss = 0.48397608\n",
      "Iteration 17, loss = 0.48394706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.48404585\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49460593\n",
      "Iteration 2, loss = 0.49209674\n",
      "Iteration 3, loss = 0.49014153\n",
      "Iteration 4, loss = 0.49148040\n",
      "Iteration 5, loss = 0.49072959\n",
      "Iteration 6, loss = 0.48869445\n",
      "Iteration 7, loss = 0.49029105\n",
      "Iteration 8, loss = 0.48937901\n",
      "Iteration 9, loss = 0.48931411\n",
      "Iteration 10, loss = 0.48898759\n",
      "Iteration 11, loss = 0.48854246\n",
      "Iteration 12, loss = 0.48814813\n",
      "Iteration 13, loss = 0.48753697\n",
      "Iteration 14, loss = 0.48777509\n",
      "Iteration 15, loss = 0.48723021\n",
      "Iteration 16, loss = 0.48684739\n",
      "Iteration 17, loss = 0.48685038\n",
      "Iteration 18, loss = 0.48619671\n",
      "Iteration 19, loss = 0.48568259\n",
      "Iteration 20, loss = 0.48635719\n",
      "Iteration 21, loss = 0.48548951\n",
      "Iteration 22, loss = 0.48531894\n",
      "Iteration 23, loss = 0.48511963\n",
      "Iteration 24, loss = 0.48409130\n",
      "Iteration 25, loss = 0.48439754\n",
      "Iteration 26, loss = 0.48395122\n",
      "Iteration 27, loss = 0.48371127\n",
      "Iteration 28, loss = 0.48359682\n",
      "Iteration 29, loss = 0.48323311\n",
      "Iteration 30, loss = 0.48324011\n",
      "Iteration 31, loss = 0.48306391\n",
      "Iteration 32, loss = 0.48304791\n",
      "Iteration 33, loss = 0.48288502\n",
      "Iteration 34, loss = 0.48301219\n",
      "Iteration 35, loss = 0.48282281\n",
      "Iteration 36, loss = 0.48281208\n",
      "Iteration 37, loss = 0.48286109\n",
      "Iteration 38, loss = 0.48279954\n",
      "Iteration 39, loss = 0.48275597\n",
      "Iteration 40, loss = 0.48289068\n",
      "Iteration 41, loss = 0.48277954\n",
      "Iteration 42, loss = 0.48274010\n",
      "Iteration 43, loss = 0.48273510\n",
      "Iteration 44, loss = 0.48266236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49884154\n",
      "Iteration 2, loss = 0.49250605\n",
      "Iteration 3, loss = 0.49031927\n",
      "Iteration 4, loss = 0.48951910\n",
      "Iteration 5, loss = 0.48971543\n",
      "Iteration 6, loss = 0.48898925\n",
      "Iteration 7, loss = 0.48848902\n",
      "Iteration 8, loss = 0.48848915\n",
      "Iteration 9, loss = 0.48823758\n",
      "Iteration 10, loss = 0.48644183\n",
      "Iteration 11, loss = 0.48680184\n",
      "Iteration 12, loss = 0.48632649\n",
      "Iteration 13, loss = 0.48638071\n",
      "Iteration 14, loss = 0.48574367\n",
      "Iteration 15, loss = 0.48576161\n",
      "Iteration 16, loss = 0.48482493\n",
      "Iteration 17, loss = 0.48490348\n",
      "Iteration 18, loss = 0.48464049\n",
      "Iteration 19, loss = 0.48461298\n",
      "Iteration 20, loss = 0.48404583\n",
      "Iteration 21, loss = 0.48406367\n",
      "Iteration 22, loss = 0.48371196\n",
      "Iteration 23, loss = 0.48352743\n",
      "Iteration 24, loss = 0.48347864\n",
      "Iteration 25, loss = 0.48361936\n",
      "Iteration 26, loss = 0.48347870\n",
      "Iteration 27, loss = 0.48330490\n",
      "Iteration 28, loss = 0.48317423\n",
      "Iteration 29, loss = 0.48324239\n",
      "Iteration 30, loss = 0.48314259\n",
      "Iteration 31, loss = 0.48308081\n",
      "Iteration 32, loss = 0.48311399\n",
      "Iteration 33, loss = 0.48293221\n",
      "Iteration 34, loss = 0.48287088\n",
      "Iteration 35, loss = 0.48291594\n",
      "Iteration 36, loss = 0.48290053\n",
      "Iteration 37, loss = 0.48284032\n",
      "Iteration 38, loss = 0.48288788\n",
      "Iteration 39, loss = 0.48294068\n",
      "Iteration 40, loss = 0.48280949\n",
      "Iteration 41, loss = 0.48282527\n",
      "Iteration 42, loss = 0.48289225\n",
      "Iteration 43, loss = 0.48281148\n",
      "Iteration 44, loss = 0.48291951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50305528\n",
      "Iteration 2, loss = 0.49118004\n",
      "Iteration 3, loss = 0.49191158\n",
      "Iteration 4, loss = 0.49235674\n",
      "Iteration 5, loss = 0.49130368\n",
      "Iteration 6, loss = 0.49137205\n",
      "Iteration 7, loss = 0.49032485\n",
      "Iteration 8, loss = 0.48954113\n",
      "Iteration 9, loss = 0.49005589\n",
      "Iteration 10, loss = 0.48922366\n",
      "Iteration 11, loss = 0.48867090\n",
      "Iteration 12, loss = 0.48816324\n",
      "Iteration 13, loss = 0.48776863\n",
      "Iteration 14, loss = 0.48724231\n",
      "Iteration 15, loss = 0.48663879\n",
      "Iteration 16, loss = 0.48634255\n",
      "Iteration 17, loss = 0.48624284\n",
      "Iteration 18, loss = 0.48565453\n",
      "Iteration 19, loss = 0.48538482\n",
      "Iteration 20, loss = 0.48516826\n",
      "Iteration 21, loss = 0.48478410\n",
      "Iteration 22, loss = 0.48464566\n",
      "Iteration 23, loss = 0.48451210\n",
      "Iteration 24, loss = 0.48422621\n",
      "Iteration 25, loss = 0.48420311\n",
      "Iteration 26, loss = 0.48392308\n",
      "Iteration 27, loss = 0.48386375\n",
      "Iteration 28, loss = 0.48390006\n",
      "Iteration 29, loss = 0.48379161\n",
      "Iteration 30, loss = 0.48376004\n",
      "Iteration 31, loss = 0.48359576\n",
      "Iteration 32, loss = 0.48366378\n",
      "Iteration 33, loss = 0.48360211\n",
      "Iteration 34, loss = 0.48368906\n",
      "Iteration 35, loss = 0.48352733\n",
      "Iteration 36, loss = 0.48358223\n",
      "Iteration 37, loss = 0.48347292\n",
      "Iteration 38, loss = 0.48351538\n",
      "Iteration 39, loss = 0.48354603\n",
      "Iteration 40, loss = 0.48349694\n",
      "Iteration 41, loss = 0.48342501\n",
      "Iteration 42, loss = 0.48346087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55350748\n",
      "Iteration 2, loss = 0.48564078\n",
      "Iteration 3, loss = 0.48470906\n",
      "Iteration 4, loss = 0.48397620\n",
      "Iteration 5, loss = 0.48382874\n",
      "Iteration 6, loss = 0.48365646\n",
      "Iteration 7, loss = 0.48356944\n",
      "Iteration 8, loss = 0.48351665\n",
      "Iteration 9, loss = 0.48347852\n",
      "Iteration 10, loss = 0.48350566\n",
      "Iteration 11, loss = 0.48340354\n",
      "Iteration 12, loss = 0.48337594\n",
      "Iteration 13, loss = 0.48338779\n",
      "Iteration 14, loss = 0.48340611\n",
      "Iteration 15, loss = 0.48338030\n",
      "Iteration 16, loss = 0.48336763\n",
      "Iteration 17, loss = 0.48343996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53702219\n",
      "Iteration 2, loss = 0.48591975\n",
      "Iteration 3, loss = 0.48467640\n",
      "Iteration 4, loss = 0.48430525\n",
      "Iteration 5, loss = 0.48398174\n",
      "Iteration 6, loss = 0.48378995\n",
      "Iteration 7, loss = 0.48377573\n",
      "Iteration 8, loss = 0.48373213\n",
      "Iteration 9, loss = 0.48367269\n",
      "Iteration 10, loss = 0.48358433\n",
      "Iteration 11, loss = 0.48353826\n",
      "Iteration 12, loss = 0.48353192\n",
      "Iteration 13, loss = 0.48361299\n",
      "Iteration 14, loss = 0.48359832\n",
      "Iteration 15, loss = 0.48357466\n",
      "Iteration 16, loss = 0.48355273\n",
      "Iteration 17, loss = 0.48352488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54492233\n",
      "Iteration 2, loss = 0.48618357\n",
      "Iteration 3, loss = 0.48526977\n",
      "Iteration 4, loss = 0.48481380\n",
      "Iteration 5, loss = 0.48448688\n",
      "Iteration 6, loss = 0.48426493\n",
      "Iteration 7, loss = 0.48422103\n",
      "Iteration 8, loss = 0.48420705\n",
      "Iteration 9, loss = 0.48418401\n",
      "Iteration 10, loss = 0.48414228\n",
      "Iteration 11, loss = 0.48415553\n",
      "Iteration 12, loss = 0.48407302\n",
      "Iteration 13, loss = 0.48406612\n",
      "Iteration 14, loss = 0.48402982\n",
      "Iteration 15, loss = 0.48401422\n",
      "Iteration 16, loss = 0.48400693\n",
      "Iteration 17, loss = 0.48399760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52044820\n",
      "Iteration 2, loss = 0.49151812\n",
      "Iteration 3, loss = 0.49081764\n",
      "Iteration 4, loss = 0.49091413\n",
      "Iteration 5, loss = 0.48893680\n",
      "Iteration 6, loss = 0.48999736\n",
      "Iteration 7, loss = 0.48955342\n",
      "Iteration 8, loss = 0.48847843\n",
      "Iteration 9, loss = 0.48842231\n",
      "Iteration 10, loss = 0.48824961\n",
      "Iteration 11, loss = 0.48738109\n",
      "Iteration 12, loss = 0.48756645\n",
      "Iteration 13, loss = 0.48641644\n",
      "Iteration 14, loss = 0.48627119\n",
      "Iteration 15, loss = 0.48591792\n",
      "Iteration 16, loss = 0.48604623\n",
      "Iteration 17, loss = 0.48557134\n",
      "Iteration 18, loss = 0.48518972\n",
      "Iteration 19, loss = 0.48487194\n",
      "Iteration 20, loss = 0.48465064\n",
      "Iteration 21, loss = 0.48438436\n",
      "Iteration 22, loss = 0.48437580\n",
      "Iteration 23, loss = 0.48395273\n",
      "Iteration 24, loss = 0.48371018\n",
      "Iteration 25, loss = 0.48357653\n",
      "Iteration 26, loss = 0.48338027\n",
      "Iteration 27, loss = 0.48322691\n",
      "Iteration 28, loss = 0.48315271\n",
      "Iteration 29, loss = 0.48315734\n",
      "Iteration 30, loss = 0.48294558\n",
      "Iteration 31, loss = 0.48294624\n",
      "Iteration 32, loss = 0.48291647\n",
      "Iteration 33, loss = 0.48276162\n",
      "Iteration 34, loss = 0.48292523\n",
      "Iteration 35, loss = 0.48283257\n",
      "Iteration 36, loss = 0.48278225\n",
      "Iteration 37, loss = 0.48275993\n",
      "Iteration 38, loss = 0.48273923\n",
      "Iteration 39, loss = 0.48280444\n",
      "Iteration 40, loss = 0.48271514\n",
      "Iteration 41, loss = 0.48273895\n",
      "Iteration 42, loss = 0.48270142\n",
      "Iteration 43, loss = 0.48271703\n",
      "Iteration 44, loss = 0.48271195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50112230\n",
      "Iteration 2, loss = 0.49120048\n",
      "Iteration 3, loss = 0.49096132\n",
      "Iteration 4, loss = 0.49033703\n",
      "Iteration 5, loss = 0.48985092\n",
      "Iteration 6, loss = 0.48896059\n",
      "Iteration 7, loss = 0.48951499\n",
      "Iteration 8, loss = 0.48803788\n",
      "Iteration 9, loss = 0.48807252\n",
      "Iteration 10, loss = 0.48799084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.48728108\n",
      "Iteration 12, loss = 0.48724404\n",
      "Iteration 13, loss = 0.48695499\n",
      "Iteration 14, loss = 0.48655780\n",
      "Iteration 15, loss = 0.48628020\n",
      "Iteration 16, loss = 0.48637256\n",
      "Iteration 17, loss = 0.48580114\n",
      "Iteration 18, loss = 0.48564853\n",
      "Iteration 19, loss = 0.48514182\n",
      "Iteration 20, loss = 0.48494490\n",
      "Iteration 21, loss = 0.48450986\n",
      "Iteration 22, loss = 0.48443572\n",
      "Iteration 23, loss = 0.48396700\n",
      "Iteration 24, loss = 0.48378400\n",
      "Iteration 25, loss = 0.48338383\n",
      "Iteration 26, loss = 0.48328742\n",
      "Iteration 27, loss = 0.48319844\n",
      "Iteration 28, loss = 0.48314314\n",
      "Iteration 29, loss = 0.48302856\n",
      "Iteration 30, loss = 0.48298903\n",
      "Iteration 31, loss = 0.48291881\n",
      "Iteration 32, loss = 0.48296001\n",
      "Iteration 33, loss = 0.48301957\n",
      "Iteration 34, loss = 0.48290028\n",
      "Iteration 35, loss = 0.48290013\n",
      "Iteration 36, loss = 0.48282523\n",
      "Iteration 37, loss = 0.48289294\n",
      "Iteration 38, loss = 0.48287086\n",
      "Iteration 39, loss = 0.48285203\n",
      "Iteration 40, loss = 0.48282397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53384631\n",
      "Iteration 2, loss = 0.49202318\n",
      "Iteration 3, loss = 0.49155628\n",
      "Iteration 4, loss = 0.49068403\n",
      "Iteration 5, loss = 0.49070938\n",
      "Iteration 6, loss = 0.49095517\n",
      "Iteration 7, loss = 0.48903474\n",
      "Iteration 8, loss = 0.49060162\n",
      "Iteration 9, loss = 0.48984005\n",
      "Iteration 10, loss = 0.48859360\n",
      "Iteration 11, loss = 0.48925595\n",
      "Iteration 12, loss = 0.48946610\n",
      "Iteration 13, loss = 0.48818975\n",
      "Iteration 14, loss = 0.48808517\n",
      "Iteration 15, loss = 0.48805748\n",
      "Iteration 16, loss = 0.48792802\n",
      "Iteration 17, loss = 0.48735767\n",
      "Iteration 18, loss = 0.48768985\n",
      "Iteration 19, loss = 0.48669635\n",
      "Iteration 20, loss = 0.48646455\n",
      "Iteration 21, loss = 0.48591548\n",
      "Iteration 22, loss = 0.48661158\n",
      "Iteration 23, loss = 0.48592370\n",
      "Iteration 24, loss = 0.48547698\n",
      "Iteration 25, loss = 0.48552440\n",
      "Iteration 26, loss = 0.48538666\n",
      "Iteration 27, loss = 0.48488640\n",
      "Iteration 28, loss = 0.48479661\n",
      "Iteration 29, loss = 0.48451708\n",
      "Iteration 30, loss = 0.48451611\n",
      "Iteration 31, loss = 0.48431093\n",
      "Iteration 32, loss = 0.48411929\n",
      "Iteration 33, loss = 0.48407660\n",
      "Iteration 34, loss = 0.48395030\n",
      "Iteration 35, loss = 0.48387386\n",
      "Iteration 36, loss = 0.48389659\n",
      "Iteration 37, loss = 0.48376783\n",
      "Iteration 38, loss = 0.48369605\n",
      "Iteration 39, loss = 0.48363535\n",
      "Iteration 40, loss = 0.48363743\n",
      "Iteration 41, loss = 0.48352027\n",
      "Iteration 42, loss = 0.48355013\n",
      "Iteration 43, loss = 0.48348397\n",
      "Iteration 44, loss = 0.48355048\n",
      "Iteration 45, loss = 0.48353917\n",
      "Iteration 46, loss = 0.48352707\n",
      "Iteration 47, loss = 0.48354188\n",
      "Iteration 48, loss = 0.48343828\n",
      "Iteration 49, loss = 0.48347839\n",
      "Iteration 50, loss = 0.48354614\n",
      "Iteration 51, loss = 0.48356167\n",
      "Iteration 52, loss = 0.48346476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53509747\n",
      "Iteration 2, loss = 0.48590854\n",
      "Iteration 3, loss = 0.48469139\n",
      "Iteration 4, loss = 0.48421194\n",
      "Iteration 5, loss = 0.48394494\n",
      "Iteration 6, loss = 0.48366199\n",
      "Iteration 7, loss = 0.48366294\n",
      "Iteration 8, loss = 0.48365709\n",
      "Iteration 9, loss = 0.48356992\n",
      "Iteration 10, loss = 0.48356153\n",
      "Iteration 11, loss = 0.48350110\n",
      "Iteration 12, loss = 0.48345936\n",
      "Iteration 13, loss = 0.48345272\n",
      "Iteration 14, loss = 0.48337609\n",
      "Iteration 15, loss = 0.48346012\n",
      "Iteration 16, loss = 0.48355217\n",
      "Iteration 17, loss = 0.48339640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54707886\n",
      "Iteration 2, loss = 0.48560329\n",
      "Iteration 3, loss = 0.48483847\n",
      "Iteration 4, loss = 0.48415208\n",
      "Iteration 5, loss = 0.48402258\n",
      "Iteration 6, loss = 0.48374300\n",
      "Iteration 7, loss = 0.48369803\n",
      "Iteration 8, loss = 0.48362217\n",
      "Iteration 9, loss = 0.48356556\n",
      "Iteration 10, loss = 0.48364420\n",
      "Iteration 11, loss = 0.48360334\n",
      "Iteration 12, loss = 0.48357319\n",
      "Iteration 13, loss = 0.48353760\n",
      "Iteration 14, loss = 0.48350930\n",
      "Iteration 15, loss = 0.48348816\n",
      "Iteration 16, loss = 0.48356143\n",
      "Iteration 17, loss = 0.48354811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55348777\n",
      "Iteration 2, loss = 0.48605046\n",
      "Iteration 3, loss = 0.48507670\n",
      "Iteration 4, loss = 0.48471740\n",
      "Iteration 5, loss = 0.48441999\n",
      "Iteration 6, loss = 0.48432049\n",
      "Iteration 7, loss = 0.48422685\n",
      "Iteration 8, loss = 0.48407121\n",
      "Iteration 9, loss = 0.48404324\n",
      "Iteration 10, loss = 0.48399373\n",
      "Iteration 11, loss = 0.48400371\n",
      "Iteration 12, loss = 0.48396842\n",
      "Iteration 13, loss = 0.48400532\n",
      "Iteration 14, loss = 0.48395370\n",
      "Iteration 15, loss = 0.48403506\n",
      "Iteration 16, loss = 0.48404387\n",
      "Iteration 17, loss = 0.48398522\n",
      "Iteration 18, loss = 0.48400904\n",
      "Iteration 19, loss = 0.48396628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53749708\n",
      "Iteration 2, loss = 0.49062783\n",
      "Iteration 3, loss = 0.48923386\n",
      "Iteration 4, loss = 0.49047182\n",
      "Iteration 5, loss = 0.48921144\n",
      "Iteration 6, loss = 0.48931787\n",
      "Iteration 7, loss = 0.48827587\n",
      "Iteration 8, loss = 0.48812869\n",
      "Iteration 9, loss = 0.48766758\n",
      "Iteration 10, loss = 0.48805374\n",
      "Iteration 11, loss = 0.48808334\n",
      "Iteration 12, loss = 0.48700197\n",
      "Iteration 13, loss = 0.48663916\n",
      "Iteration 14, loss = 0.48658902\n",
      "Iteration 15, loss = 0.48644410\n",
      "Iteration 16, loss = 0.48584586\n",
      "Iteration 17, loss = 0.48550520\n",
      "Iteration 18, loss = 0.48498595\n",
      "Iteration 19, loss = 0.48504089\n",
      "Iteration 20, loss = 0.48454914\n",
      "Iteration 21, loss = 0.48465229\n",
      "Iteration 22, loss = 0.48418496\n",
      "Iteration 23, loss = 0.48409426\n",
      "Iteration 24, loss = 0.48358932\n",
      "Iteration 25, loss = 0.48353781\n",
      "Iteration 26, loss = 0.48335820\n",
      "Iteration 27, loss = 0.48316449\n",
      "Iteration 28, loss = 0.48309234\n",
      "Iteration 29, loss = 0.48298263\n",
      "Iteration 30, loss = 0.48286251\n",
      "Iteration 31, loss = 0.48290570\n",
      "Iteration 32, loss = 0.48281273\n",
      "Iteration 33, loss = 0.48273848\n",
      "Iteration 34, loss = 0.48274817\n",
      "Iteration 35, loss = 0.48280516\n",
      "Iteration 36, loss = 0.48274146\n",
      "Iteration 37, loss = 0.48280542\n",
      "Iteration 38, loss = 0.48282043\n",
      "Iteration 39, loss = 0.48274216\n",
      "Iteration 40, loss = 0.48270258\n",
      "Iteration 41, loss = 0.48280062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49345659\n",
      "Iteration 2, loss = 0.49185528\n",
      "Iteration 3, loss = 0.49141639\n",
      "Iteration 4, loss = 0.49112660\n",
      "Iteration 5, loss = 0.49065678\n",
      "Iteration 6, loss = 0.48976053\n",
      "Iteration 7, loss = 0.48944943\n",
      "Iteration 8, loss = 0.48907483\n",
      "Iteration 9, loss = 0.48850680\n",
      "Iteration 10, loss = 0.48881471\n",
      "Iteration 11, loss = 0.48785135\n",
      "Iteration 12, loss = 0.48714620\n",
      "Iteration 13, loss = 0.48719498\n",
      "Iteration 14, loss = 0.48698001\n",
      "Iteration 15, loss = 0.48671574\n",
      "Iteration 16, loss = 0.48607194\n",
      "Iteration 17, loss = 0.48620303\n",
      "Iteration 18, loss = 0.48571371\n",
      "Iteration 19, loss = 0.48545910\n",
      "Iteration 20, loss = 0.48495909\n",
      "Iteration 21, loss = 0.48527971\n",
      "Iteration 22, loss = 0.48482454\n",
      "Iteration 23, loss = 0.48480386\n",
      "Iteration 24, loss = 0.48455755\n",
      "Iteration 25, loss = 0.48427781\n",
      "Iteration 26, loss = 0.48376129\n",
      "Iteration 27, loss = 0.48388192\n",
      "Iteration 28, loss = 0.48372055\n",
      "Iteration 29, loss = 0.48350374\n",
      "Iteration 30, loss = 0.48336352\n",
      "Iteration 31, loss = 0.48333300\n",
      "Iteration 32, loss = 0.48315777\n",
      "Iteration 33, loss = 0.48312158\n",
      "Iteration 34, loss = 0.48311864\n",
      "Iteration 35, loss = 0.48306190\n",
      "Iteration 36, loss = 0.48305176\n",
      "Iteration 37, loss = 0.48294064\n",
      "Iteration 38, loss = 0.48295417\n",
      "Iteration 39, loss = 0.48292374\n",
      "Iteration 40, loss = 0.48287170\n",
      "Iteration 41, loss = 0.48291455\n",
      "Iteration 42, loss = 0.48298841\n",
      "Iteration 43, loss = 0.48290718\n",
      "Iteration 44, loss = 0.48294311\n",
      "Iteration 45, loss = 0.48283723\n",
      "Iteration 46, loss = 0.48286646\n",
      "Iteration 47, loss = 0.48290229\n",
      "Iteration 48, loss = 0.48280823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49390446\n",
      "Iteration 2, loss = 0.49213996\n",
      "Iteration 3, loss = 0.49240193\n",
      "Iteration 4, loss = 0.49236118\n",
      "Iteration 5, loss = 0.49055424\n",
      "Iteration 6, loss = 0.48985431\n",
      "Iteration 7, loss = 0.49021762\n",
      "Iteration 8, loss = 0.48895030\n",
      "Iteration 9, loss = 0.48862649\n",
      "Iteration 10, loss = 0.48839882\n",
      "Iteration 11, loss = 0.48855438\n",
      "Iteration 12, loss = 0.48815988\n",
      "Iteration 13, loss = 0.48836897\n",
      "Iteration 14, loss = 0.48727964\n",
      "Iteration 15, loss = 0.48824090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.48715318\n",
      "Iteration 17, loss = 0.48688977\n",
      "Iteration 18, loss = 0.48657520\n",
      "Iteration 19, loss = 0.48677225\n",
      "Iteration 20, loss = 0.48666261\n",
      "Iteration 21, loss = 0.48633472\n",
      "Iteration 22, loss = 0.48599993\n",
      "Iteration 23, loss = 0.48554228\n",
      "Iteration 24, loss = 0.48539308\n",
      "Iteration 25, loss = 0.48544671\n",
      "Iteration 26, loss = 0.48501901\n",
      "Iteration 27, loss = 0.48470509\n",
      "Iteration 28, loss = 0.48468211\n",
      "Iteration 29, loss = 0.48441877\n",
      "Iteration 30, loss = 0.48414302\n",
      "Iteration 31, loss = 0.48394819\n",
      "Iteration 32, loss = 0.48393429\n",
      "Iteration 33, loss = 0.48378772\n",
      "Iteration 34, loss = 0.48364037\n",
      "Iteration 35, loss = 0.48369668\n",
      "Iteration 36, loss = 0.48365542\n",
      "Iteration 37, loss = 0.48357634\n",
      "Iteration 38, loss = 0.48347832\n",
      "Iteration 39, loss = 0.48356342\n",
      "Iteration 40, loss = 0.48348641\n",
      "Iteration 41, loss = 0.48359002\n",
      "Iteration 42, loss = 0.48345543\n",
      "Iteration 43, loss = 0.48347410\n",
      "Iteration 44, loss = 0.48349832\n",
      "Iteration 45, loss = 0.48347796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4e82cabcc554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \"\"\"\n\u001b[1;32m    994\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 995\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[0;32m--> 375\u001b[0;31m                             intercept_grads, layer_units)\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    467\u001b[0m                     \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 },\n\u001b[0;32m--> 469\u001b[0;31m                 args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 600\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[0;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[0;32m--> 177\u001b[0;31m             X, y, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Iterate over the hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0minplace_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDERIVATIVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0minplace_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_feat = data[['LATITUDE',\"LONGITUDE\",'NUMTIME','BRONX','BROOKLYN','MANHATTAN','QUEENS','STATEN ISLAND','Environmental','Human','Unspecified','Vehicular']]\n",
    "y = resp\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feat, y, test_size=0.2)\n",
    "\n",
    "dtc = MLPClassifier(verbose=True)\n",
    "grid = {'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'max_iter': [100,200,500,1000,2000]}\n",
    "\n",
    "gs = GridSearchCV(dtc,grid,cv=3,verbose=True,return_train_score=False)\n",
    "gs.fit(X_feat,y)\n",
    "\n",
    "scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
    "                                            ).sort_values('mean_test_score',\n",
    "                                                          ascending=False).reset_index().drop(['index'],axis=1)\n",
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate (Sensitivity)')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_feat = pred\n",
    "\n",
    "y = resp\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feat, y, test_size=0.2)\n",
    "\n",
    "\n",
    "t2 = RandomForestClassifier(n_estimators=100,max_depth=4,min_samples_leaf=20)\n",
    "\n",
    "t2.fit(X_train,y_train)\n",
    "y_test_pred = t2.predict_proba(X_test)[:,1]\n",
    "new = np.concatenate((y_test.values.reshape(-1,1),y_test_pred.reshape(-1,1)),axis=1)\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = metrics.roc_curve(y_test, y_test_pred)\n",
    "\n",
    "sns.mpl.pyplot.plot(fpr_rf, tpr_rf,label=\"rf\")\n",
    "sns.mpl.pyplot.xlim([0, 1])\n",
    "sns.mpl.pyplot.ylim([0, 1.05])\n",
    "sns.mpl.pyplot.legend(loc=\"lower right\")\n",
    "sns.mpl.pyplot.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.mpl.pyplot.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF model AUC:  0.57890532173941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = metrics.roc_curve(y_test, y_test_pred)\n",
    "print(\"RF model AUC: \",metrics.roc_auc_score(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate (Sensitivity)')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_test_pred = t2.predict_proba(X_test)[:,1]\n",
    "new = np.concatenate((y_test.values.reshape(-1,1),y_test_pred.reshape(-1,1)),axis=1)\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = metrics.roc_curve(y_test, y_test_pred)\n",
    "sns.mpl.pyplot.plot(fpr_rf, tpr_rf,label=\"lr\")\n",
    "sns.mpl.pyplot.plot(fpr_rf, thresholds_rf,label=\"rf\")\n",
    "sns.mpl.pyplot.xlim([0, 1])\n",
    "sns.mpl.pyplot.ylim([0, 1.05])\n",
    "sns.mpl.pyplot.legend(loc=\"lower right\")\n",
    "sns.mpl.pyplot.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.mpl.pyplot.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
